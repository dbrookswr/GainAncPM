\documentclass[man, mask, floatsintext]{apa6}
\usepackage{apacite}
\usepackage{natbib}
\setlength{\bibsep}{0.0pt}
\usepackage{epstopdf}% To incorporate .eps 
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{arydshln}
\usepackage{graphicx,psfrag,epsf}
\usepackage{setspace}
\usepackage{enumitem}  
\usepackage{multirow}
\usepackage{wasysym}
\setlist[enumerate]{noitemsep}
\usepackage{pgfplots}
\pgfplotsset{compat=1.5}
\usepackage{comment}
\usepackage{url}  
\usepackage{tikz}
\usetikzlibrary{arrows,shapes,positioning,fit,arrows.meta}
%\tikzset{>={Latex[width=01.5mm,length=01.5mm]}}
%\tikzset{>={Stealth[scale = 1.3]}}
\title{Deciding among Gain Scores, Ancova, and Propensity Matching Procedures}
\shorttitle{Gains, Covariates, and Propensity}
\author{Daniel B.~Wright}
\affiliation{University of Nevada, Las Vegas}
\authornote{ Department of Educational Psychology \& Higher Education, College of Education, University of Nevada, Las Vegas. Daniel Wright is the Dunn Family Endowed Chair and Professor of Educational Assessment. This research received no funding beyond the endowment.\\
Email: daniel.wright@unlv.edu or dbrookswr@gmail.com}
\abstract{Researchers have several options available to analyze data from interventions when participants have not been randomly allocated into conditions. Among these are the gain score, Ancova, and propensity matching procedures. Each of these attempts to account for differences among the conditions, but they do so differently. After reviewing these procedures, causal models are hypothesized for how data for an intervention may arise, the data simulated many times, and these procedures applied to each of these replications. For some situations the gain score procedure produced biased estimates (e.g., when the covariate influences group allocation) and the Ancova and propensity matching procedures produced less biased results. Other situations (e.g., when the covariate did not influence treatment) the opposite occurred.  For the conditions tested, the propensity matching procedure outperformed Ancova. The main conclusion is that models should be hypothesized for how the data may arise, data simulated for these models, and the properties of statistical procedures evaluated. None of these procedures will provide accurate estimates for all situations.  
\vspace{1cm}}
\keywords{Propensity Matching, Ancova, Gain scores, Lord's paradox, Causality, Graphs, Simulation}

\begin{document}
\maketitle

When researchers propose a new treatment, it is important to evaluate whether it is beneficial. If people have been randomly allocated into conditions researchers can compare the treatment group with a control group on some outcome variable. In much psychology it is not possible to allocate people randomly into conditions: a health psychologist cannot force someone to be pregnant; a neuropsychologist cannot injure someone; an economic psychologist cannot force a company to commit fraud, \emph{etc}. This is true in many disciplines (e.g., an astronomer cannot assign a star to go supernova; a historian cannot randomly decide whether Trotsky or Stalin succeeds Lenin; see also the evaluations at \url{www.povertyactionlab.org}).

Because of this researchers rely on statistical techniques with the belief that these techniques somehow ``control'' the effects of other variables, usually called covariates. These procedures do not physically ``control'' or in any other way effect these covariates. Unfortunately this falsehood has misled generations of students.  Three of these procedures will be discussed: gain scores, Ancova, and propensity matching. All of these provide accurate solutions in certain circumstances to certain questions. The difficulty is knowing when each of these, and if any of these, is appropriate. These methods are described in more detail. Graphical models and simulation are used to explore when they produce biased estimates. 

\section{Example: Evaluation when there is a collider}
An example intervention will be used throughout most of this paper. It was chosen to represent evaluations where the researcher wants to know if a treatment works, has a prior score measured before the treatment that is on the same scale as the outcome measure, but cannot randomly allocate people into a treatment and a control condition. There are many issues researchers face. The focus for this example is on colliders, which are explained below. It is important to stress, however, that the basic steps: 
\begin{enumerate}[noitemsep]
\item describe models for how the data might arise, 
\item simulate data according to these models, and 
\item evaluate different statistical procedures, 
\end{enumerate}
\noindent can be applied to all interventions.

Suppose you want to assess whether year-long weekly after school math clubs improve students' scores at the end of the year. Denote being in the club with $\mathit{Treatment} = 1$ (0 otherwise) and the end of year assessment with $\mathit{PostTest}$ (subscripts will not be used on variables in text or in figures, but all vary by participant). There would be complaints if students were randomly assigned to this intervention, so instead students volunteer for it. Who volunteers for an after school math club is not random. Assume there is some latent variable, call it $\mathit{Propensity}$, that predicts whether someone volunteers. $\mathit{Propensity}$ is likely also to influence many other things, including how much math knowledge the student has already learned. Call this $\mathit{Knowledge}$. $\mathit{Knowledge}$ will also be affected by other constructs including the student's intelligence, effort/grit, home environment, \emph{etc}. Call this collection of constructs $\mathit{Other}$. 

\subsection{Graphs, Causal Models, and Colliders}
It is often useful to draw relationships as a graph \citep[not everyone agrees, see][p.~22]{ImbensRubin2015}. A graph, in its mathematical sense, is a set of nodes, some of which are connected by edges. In causal models these edges are often shown with an arrow on one end to denote the direction of causality. The classic reference applying graphs to causal models in science is \citet{Pearl2009}, and good introductions include: \citet{Elwart2013}, \citet{MorganWinship2007}, and \citet{PearlEA2016}. Figure~\ref{fig:datamodel} shows the model described above (each variable also includes an error term, which are not shown in order to make the figures less cluttered). Models are simplifications. For example, it may be that $\mathit{Propensity}$ and $\mathit{Other}$ are causally related, but this is not shown here. The primary interest is estimating the edge $\mathit{Treatment} \rightarrow \mathit{PostTest}$, enclosed with a red ellipse. This is the direct path. There are also backdoor paths between these nodes. These are:
\begin{enumerate}[noitemsep]
\item \label{path1} $\mathit{Treatment} \leftarrow \mathit{Propensity} \rightarrow \mathit{Knowledge} \rightarrow \mathit{PostTest}$
\item \label{path2} $\mathit{Treatment} \leftarrow \mathit{Propensity} \rightarrow \mathit{Knowledge} \leftarrow \mathit{Other} \rightarrow \mathit{PostTest}$
\end{enumerate}

\begin{figure}
\centering
\begin{tikzpicture}[scale=0.8, every node/.style={transform shape}]
%  \node[scale=1.4] (A) at (-3,14) {\textbf{A}};
  \node[scale=1.2,draw,ellipse] (propen) at (-1,13) {\emph{Propensity}};
%  \node[scale=1.2,draw,rectangle] (treat) at (-1,10) {\emph{Treatment}};
  \node[scale=1.2] (treat) at (-1,10) {\emph{Treatment}};
  \node[scale=1.2,draw,ellipse] (know) at (2,11.5) {\emph{Knowledge}};
  \node[scale=1.2,draw,ellipse] (other) at (5,13) {\emph{Other}};
  \node[scale=1.2] (post) at (2,8) {\emph{PostTest}};
%  \node[scale=1.2,draw,rectangle] (post) at (2,8) {\emph{PostTest}};
  \draw[shorten >=0.0cm,shorten <=.0cm,->](propen) -- (treat);
  \draw[shorten >=0.1cm,shorten <=.0cm,->](propen) -- (know);
  \draw[shorten >=0.2cm,shorten <=.2cm,->](treat) -- (post);
  \draw[shorten >=0.0cm,shorten <=.0cm,->](know) -- (post);
  \draw[shorten >=0.0cm,shorten <=.0cm,->](other) -- (2.78,8.31);
  \draw[shorten >=0.1cm,shorten <=.0cm,->](other) -- (know);
    \draw[rotate around={57.0:(.5,9)}, red] (.5,9) ellipse (7pt and 35pt);
  \node[scale=1.2] (spacebottom) at (1,7.5) {\phantom{Hello}};
  \node[scale=1.2] (spacetop) at (1,13.5) {\phantom{Hello}};
\end{tikzpicture}
\caption{\label{fig:datamodel} The data model of the latent variables, outcome variable, and treatment. The direct effect from \emph{Treatment} to \emph{PostTest} is enclosed in the {\color{red}red ellipse}.}
\end{figure}


Paths can be either blocked or unblocked. If they are unblocked it means information can flow along them confounding measurement of the direct path. Therefore, often a goal of choosing covariates is to block backdoor paths. How detrimental the effects of an unblocked door path are depends on the product of the path coefficients \citep{Loehlin1998}. To understand blocking paths it is necessary to consider three ways in which three variables, call them $X$, $Y$, and $Z$, can be causally related:

\begin{description}[noitemsep]
\item{Chain:} $X \rightarrow Y \rightarrow Z$,
\item{Fork:} $X \leftarrow Y \rightarrow Z$, and 
\item{Collider:} $X \rightarrow Y \leftarrow Z$.
\end{description}
\noindent \citet[pp.~16--17]{Pearl2009} describes two rules to determine if the path between two nodes, $X$ and $Z$, is blocked and how this is affected by conditioning on the middle variable $Y$. 

Pearl's first rule is that if a path contains a chain or a fork, it is unblocked unless the middle variable is conditioned upon. Chains and forks are associated with the phrases mediation and spurious correlation in the psychology literature. An example of an unblocked chain is that an exercise pamphlet can lead to students planning to exercise and this can lead to more exercise \citep{HillEA2007}. If you prevent students from the planning phase then giving participants a pamphlet is not as effective. A common textbook example of a fork is the positive association between ice cream consumption and murder in cities \citep{Peters2013}. The warm weather causes both of these to increase. If you could condition on the weather by looking just at one particular weather (e.g., sunny and 83$^{\circ}$F), this would block the path and assuming no other effects are present, the correlation would be near zero. In Figure~\ref{fig:datamodel}, the path $\mathit{Treatment} \leftarrow \mathit{Propensity} \rightarrow \mathit{Knowledge} \rightarrow \mathit{PostTest}$ includes a chain (middle variable $\mathit{Knowledge}$) and a fork (middle variable $\mathit{Propensity}$) and therefore begins unblocked. Much of the justification for using covariates is to block paths like this. One way to block this path would be to condition upon \emph{Propensity} or \emph{Knowledge}, though because these are latent variables it is not possible to condition upon them. Instead researchers use measured variables that are associated with them. Unless the association between the latent and observed variable is perfect, this conditioning does not eliminate the flow of information (so does not completely block the path), but does lessen the amount of information flow. 

Pearl's second rule is that a path with a collider begins block, but is unblocked if the collider, or any variable influenced by it (called \emph{descendants} in graph theory terminology), is conditioned upon. Colliders are less discussed in the psychological literature than forks and chains. \citet{Wright2017vam} uses a river metaphor to describe a collider. Imagine two tributaries arriving from different directions at a deep sink hole. The hole is the collider. The water from each would not reach the other; the path is blocked. If the hole is filled, water could flow between the tributaries and the path would be unblocked. A common example of a collider affecting the measurement of a direct effect is the smoking-birth weight paradox \citep[e.g.,][]{PearlEA2016,Wright2018SGP}. This paradox refers to the finding that smoking appears to \emph{lower} infant mortality rates \emph{if} you condition on the infants' weight. The reason for this is that low birth weight is influenced by smoking, but it is also influenced by a lot of other things, some of which are much worse than smoking with respect to infant mortality. Therefore, conditioning compares infants with low birth weight due to smoking with infants with low birth weight from these other causes, and smoking is relatively less detrimental. Birth weight is a collider in the path $\mathit{Smokes} \rightarrow \mathit{Low Birth Weight} \leftarrow \mathit{Other} \rightarrow \mathit{Mortality}$ because it is influenced by both smoking and other causes. In Figure~\ref{fig:datamodel} the backdoor path $\mathit{Treatment} \leftarrow \mathit{Propensity} \rightarrow \mathit{Knowledge} \leftarrow \mathit{Other} \rightarrow \mathit{PostTest}$ has a collider, $\mathit{Knowledge}$, so this path begins blocked. Unfortunately one method for blocking the first path (conditioning on \emph{Knowledge}) will unblock this path and conditioning on descendants of this collider will also unblock this path.  

In the typical social and behavioral science project, particularly one where random allocation is not possible, it is common to measure several  other variables with the hope of blocking (and keeping blocked) all backdoor paths thereby isolating the direct effect. Here it is assumed that there are three measured variables that are related to \emph{Propensity}, three to \emph{Knowledge}, and three to \emph{Other}. For the simulation each of these observed variables is correlated $r = .50$ with their associated latent variable, but how they are associated with the latent variable differs. One influences the latent variable (e.g., $\mathit{p1} \rightarrow \mathit{Propensity}$), one is influenced by the latent variable (e.g., $\mathit{Prior} \leftarrow \mathit{Knowledge}$), and for the other in the trio another variable (depicted just with a circle) influences both (e.g., $\mathit{o3} \leftarrow \fullmoon \rightarrow \mathit{Other}$). The corresponding graph is shown in Figure~\ref{fig:datamodel2}. While this model looks complex, like all models in social science it is still a simplification. Several of the nodes listed likely influence others (e.g., $\mathit{Other} \rightarrow \mathit{Propensity}$), there are many other constructs that play a role, and there are many other variables that could be measured. After the main simulation some extensions will be considered. It is worth noting that observed variables are not placed along the paths from \emph{Treatment}, \emph{Knowledge}, and \emph{Other} to \emph{PostTest}, and there are no nodes just affecting \emph{Treatment}. These effects could provide additional ways to measure the treatment effect. There are several ways to address measurement of interventions and readers are encouraged to consult textbooks devoted to this \citep[e.g.,][]{ImbensRubin2015,MorganWinship2007,PearlEA2016}.


\begin{figure}
\centering
\begin{tikzpicture}
  \node[scale=1,draw,ellipse] (propen2) at (8.3,11) {\emph{Propensity}};
  \node[scale=1] (p1) at (9.3,12.2) {$p_1$};
  \node[scale=1] (p2) at (8.3,12.2) {$p_2$};
  \node[draw, ellipse, scale=.7] (phantomp) at (6.8,12.2) {\phantom{W}};
  \node[scale=1] (p3) at (6.3,11) {$p_3$};
  \node[scale=1] (treat2) at (8.3,7.7) {\emph{Treatment}};
  \node[scale=1,draw,ellipse] (know2) at (12,9.5) {\emph{Knowledge}};
  \node[scale=1] (x1) at (11,10.7) {$x_1$};
  \node[scale=1] (Prior) at (12.4,10.7) {\emph{Prior}};
  \node[draw, ellipse, scale=.7] (phantomx) at (9.5,9.5) {\phantom{W}};
  \node[scale=1] (x3) at (9.5,8.5) {$x_3$};
  \node[scale=1,draw,ellipse] (other2) at (15.2,11) {\emph{Other}};
  \node[scale=1] (o1) at (14.2,12.2) {$o_1$};
  \node[scale=1] (o2) at (15.2,12.2) {$o_2$};
  \node[draw, ellipse, scale=.7] (phantomo) at (16.2,12.2) {\phantom{W}};
  \node[scale=1] (o3) at (16.7,11) {$o_3$};
\node[scale=1.2] (spacebottom) at (10,5.5) {\phantom{Hello}};
  


  \node[scale=1] (post2) at (12,6) {\emph{PostTest}};
  \draw[shorten >=0.0cm,shorten <=.0cm,->](propen2) -- (treat2);
  \draw[shorten >=0.1cm,shorten <=.0cm,->](propen2) -- (know2);
  \draw[shorten >=0.25cm,shorten <=.2cm,->](treat2) -- (post2);
  \draw[shorten >=0.0cm,shorten <=.0cm,->](know2) -- (post2);
  \draw[shorten >=0.0cm,shorten <=.0cm,->](other2) -- (12.5,6.27);
  \draw[shorten >=0.1cm,shorten <=.0cm,->](other2) -- (know2);
  
  \draw[shorten >=0.1cm,shorten <=.0cm,->](p1) -- (propen2);
  \draw[shorten >=0.1cm,shorten <=.0cm,->](phantomp) -- (propen2);
  \draw[shorten >=0.0cm,shorten <=.0cm,->](phantomp) -- (p3);
  \draw[shorten >=0.0cm,shorten <=.0cm,->](propen2) -- (p2);

  \draw[shorten >=0.1cm,shorten <=.0cm,->](x1) -- (know2);
  \draw[shorten >=0.1cm,shorten <=.0cm,->](phantomx) -- (know2);
  \draw[shorten >=0.0cm,shorten <=.0cm,->](phantomx) -- (x3);
  \draw[shorten >=0.0cm,shorten <=.0cm,->](know2) -- (Prior);
  
  \draw[shorten >=0.1cm,shorten <=.0cm,->](o1) -- (other2);
  \draw[shorten >=0.1cm,shorten <=.0cm,->](phantomo) -- (other2);
  \draw[shorten >=0.0cm,shorten <=.0cm,->](phantomo) -- (o3);
  \draw[shorten >=0.0cm,shorten <=.0cm,->](other2) -- (o2);
  \draw[rotate around={66:(10.15,6.85)}, red] (10.15,6.85) ellipse (7pt and 37pt);

\end{tikzpicture}
\caption{\label{fig:datamodel2} The data model with both latent and observed variables. The direct effect from \emph{Treatment} to \emph{PostTest}, enclosed in the {\color{red}red ellipse}, is the construct the researchers wish to measure accurately. To make the figure less cluttered the individual node error variables are not shown.}
\end{figure}

The variable \emph{Prior} is named in Figure~\ref{fig:datamodel2} as opposed to just calling it $x2$ because of its central role in the gain score model. It is assumed that it is on the same scale as \emph{PostTest} (in the simulation both have means of zero and standard deviations of one) so that $\mathit{PostTest} - \mathit{Prior}$ is meaningful and that this difference represents the same \emph{gain} throughout the span of \emph{Prior}. \emph{Prior} is influenced by \emph{Knowledge}; it is a descendant of \emph{Knowledge}. Therefore according to Pearl's second rule, conditioning on it unblocks the path $\mathit{Treatment} \leftarrow \mathit{Propensity} \rightarrow \mathit{Knowledge} \leftarrow \mathit{Other} \rightarrow \mathit{PostTest}$.

\subsection{Analytic Strategies}
Comparing scores on $\mathit{PostTest}$ by \emph{Treatment} using something like a $t$-test is not a good measure of the intervention when the two groups begin systematically different in ways associated with the outcome variable because differences on the outcome variable could be due to either the intervention or the pre-existing differences. Three procedures will be considered in this paper: gain scores, Ancova, and propensity matching. Each of these is sometimes described as ``controlling for previous performance.''  

The first two procedures are often discussed with reference to Lord's paradox (\citeyear{Lord1967,Lord1969}). Lord described a situation: looking at students' weights, before and after a year of college, where interest was with the gender difference. Lord imagined two statisticians proposing different methods for the analysis. The first statistician proposed subtracting the two weights and comparing means of these gain scores. The second statistician proposed predicting final weight from gender after conditioning by the initial weight using an Ancova. The statisticians came up with different results (the first statistician found no difference in weight gain, the second found males weighed more than females after conditioning on pre-weights). Several authors have shown when and why these approaches can produce different effects \citep[e.g.,][]{Hand1994,HollandRubin1983,Pearl2016,Wainer1991,Wright2020lord}. Since Lord described this paradox, propensity matching \citep[e.g.][]{Rosenbaum2002,Rubin2006} has become very popular, though many express concern that it is being used without due concern \citep[e.g.,][]{Pearl2009,Sekhon2009}. Therefore it will also be compared.

\paragraph{Gain scores} 
The simplest of the procedures considered, computationally, is the gain score method. Let $\mathit{Gain} = \mathit{PostTest} - \mathit{Prior}$ and it is assumed that these scores have approximately the same meaning for each level of $\mathit{Prior}$. Analyses can then be conducted on this variable using $\mathit{Treatment}$ with or without the other observed variables: $p_{1} \dots o_{3}$ (shown with the $\cdots$ in eqn.~\ref{eqn:gain} below). Lord's (1967) first statistician conducted a $t$-test between the two groups on the gain score (i.e., no other covariates). One limitation of this approach is that it must make sense to equate, for example, Tom's increase from 96 to 99 with Jerry's increase from 47 to 50. In regression format this would be testing the hypothesis $\beta_1 = 0$ in:
\begin{equation}\label{eqn:gain}
\mathit{Gain_i} = \mathit{PostTest_i} - \mathit{Prior_i} = \beta_0 + \beta_1 \, \mathit{Treatment_i} + \cdots + e_i \quad .
\end{equation}
\noindent In \textsf{R}, where \texttt{Covs} is all covariates other than treatment and initial score, this would be:
<<eval=FALSE>>=
lm(PostTest - Prior ~ Treatment + Covs)
@

\paragraph{Ancova}
Lord's (1967) second statistician conducted an Ancova. This is the procedure most often described as \emph{controling} covariates. Some people believe this, what \citet{Braun2013} describes as magical thinking. There have been decades of warnings about the limitations of this procedure \citep[e.g.,][]{Kahneman1965,Meehl1970}. The phrase Ancova can mean different things to different people, but here it will refer to the model in eqn.~\ref{eqn:ancova}:
\begin{equation} \label{eqn:ancova}
\mathit{PostTest_i} = \beta_0 + \beta_1 \, \mathit{Treatment_i} + \beta_2 \, \mathit{Prior_i} + \dots + e_i \quad .
\end{equation}
This Ancova tests if $\beta_1 = 0$, like eqn.~\ref{eqn:gain}, but this $\beta_1$ is different. It is the effect after conditioning on \emph{Prior} and the outcome variable is different (note that if $\beta_2$ is fixed at one this will fitted identically as eqn.~\ref{eqn:gain}). More covariates are often added to the regression, further obfuscating the meaning of $\beta_1$, sometimes with the optimistic hope that by including lots of variables this somehow gets closer to the isolating the influence of $\mathit{Treatment}$ on $\mathit{PostTest}$. Including some covariates can make this more realistic, but including others will make it less realistic (e.g., conditioning on a collider in a backdoor path). Adding or removing a covariate changes the meaning of the parameter being estimated. The importance of thinking carefully about which covariates to include based on their role in the overall causal model and evaluating the performance of these statistical models with simulated data sets are the take-home messages of this paper. In \textsf{R}, where \texttt{Covs} is all covariates other than treatment, this would be:
<<eval=FALSE>>=
lm(PostTest ~ Treatment + Prior + Covs)
@

\paragraph{Propensity Matching}
Trying to reach causal conclusions when people are not randomly allocated into groups is difficult. Propensity matching attempts to create an accurate model of who chooses (or is chosen) to be in the intervention. If two people have equal probabilities of being in the intervention, and if they differ in no other ways that are associated with the outcome and covariates, then which condition that they are in is effectively a flip of coin. The ``differ in no other ways \dots '' is an assumption that can be difficult to justify. Propensity matching was developed in a series of papers by Rosenbaum and Rubin. The seminal textbook is \citet{Rosenbaum2002} and many of their contributions have been re-published in \citet{Rubin2006}. There are several statistics packages for performing propensity matching and estimating the treatment effects (see \url{www.biostat.jhsph.edu/~estuart/propensityscoresoftware.html}). \textsf{R} will be used for the simulation so Keller and Tipton's (\citeyear{KellerTipton2016}) review of \textsf{R} propensity matching packages is particularly relevant.

Propensity matching can be divided into the following steps:
\begin{enumerate}[noitemsep]
\item Choose a set of variables to estimate the probability of being in the treatment. This is the most important step and requires much content knowledge about how the variables are likely to relate. 
\item Estimate the probability of being in the intervention, based on these covariates. Often this is done with a logistic regression, but there are many other possibilities. 
\item Create matched samples of treatment and control participants. Several algorithms can be used for this, and particularly when the treatment is expensive/rare, it is common to match many control participants to each treatment participant.
\item Compare these matched samples to estimate the treatment effect. There are some options here, but for current purposes this will be the average treatment effect.
\end{enumerate}

This can be implemented in \textsf{R} in many ways. Here \texttt{Covs} may or may not include \texttt{Prior}, depending on what the analyst believes. In the simulation the following is used:

\begin{singlespace}
<<showpm,eval=FALSE>>=
propval <- glm(Treatment ~ Covs,family=binomial)$fitted
library(Matching)
Match(PostTest,Treatment,propval,
      estimand="ATE",BiasAdjust=TRUE,ties=TRUE)
@
\end{singlespace}

\texttt{Match} is a function from the \textbf{Matching} package \citep{Matching} and the code on the second line of this function call are specific options for this function. The only of note is \texttt{estimand="ATE"}, which means the estimate is the average treatment effect. The user also has the option to estimate the treatment effect for those in the treatment or for those in the control group. These would be valuable for different applied problems. The former to estimate the effect for those likely to sign up for the treatment and the later the value if you could encourage those who would not normally sign up to sign up.

There are differences between propensity matching and Ancova (e.g., that matching is used for conditioning in one). The most important difference for the current paper is that with Ancova the covariates are used to predict the outcome (so models might maximize $R^2$ for the final test scores) while with propensity matching the covariates are used to predict being in the treatment condition. The goal for propensity is to have matched cases with equal probability of being in the treatment group. If it can be argued that there are two people each with 62\% chance of being in the math club, and no other differences, then it can be argued that this is like an experiment. In practice, there are likely other differences among those in the math clubs than those not. \citet{Rosenbaum2002} describes methods to examine this and many of these are implemented in the available software, but there may be differences that are not captured by any of the observed variables.

\subsection{Simulation Methods}
Simulation methods are used here to explore the choice of statistical procedures and which covariates to include. These methods allow data for causal models to be created several thousand times and different statistical methods applied and evaluated. They also allow the causal models to be systematically manipulated to examine further research questions. For example, the path $\mathit{Other} \rightarrow \mathit{PostTest}$ can be removed (which would mean \emph{Knowledge} is no longer a collider in a backdoor path, and this is considered in the first extension below) and different statistical models evaluated. The \textsf{R} statistics environment \citep[version 3.6.1]{R} is used. Other environments/packages (e.g., Python, SAS, SPSS) could have been used, but using \textsf{R} allows use of its propensity matching packages \citep[e.g.,][]{KellerTipton2016} and it is freely available to all readers. 

Eleven sets of covariates were used for each procedure. These were: a model without any covariates (except for the propensity matching procedures, since the propensities would all be the same so the matching algorithm will not work), each of the nine covariates ($p1 \dots o3$) on its own (except using \emph{Prior} as a covariate with the gain score model since, as noted above, this would be equivalent to the Ancova), and all of the covariates. The gain score and Ancova models both used \textsf{R}'s \texttt{lm} function. Propensity matching used \textsf{R}'s \texttt{glm} function (logistic regression) to calculate propensity scores and the \texttt{Match} function \citep{Matching} to estimate the average treatment effect.

<<packages,echo=FALSE>>=
#opts_chunk$set(cache.path = "C:\\Users\\dwright\\Documents\\Vam Research\\cache\\")
suppressPackageStartupMessages(library(xtable))
suppressPackageStartupMessages(library(Matching))
suppressPackageStartupMessages(library(recoder))
@

<<allmodels,echo=FALSE>>=
# 0 gain, 1 ancova, 2 pm
gain <- function(p1,p2,p3,x1,prior,x3,o1,o2,o3,treat,post,know){
     gainvals <- matrix(ncol=6,nrow=12)
     gainvals[1,] <- c(0,1,summary(lm(post - prior ~ treat))$coef[2,])
     gainvals[2,] <- c(0,2,summary(lm(post - prior ~ treat + p1))$coef[2,])
     gainvals[3,] <- c(0,3,summary(lm(post - prior ~ treat + p2))$coef[2,])
     gainvals[4,] <- c(0,4,summary(lm(post - prior ~ treat + p3))$coef[2,])
     gainvals[5,] <- c(0,5,summary(lm(post - prior ~ treat + x1))$coef[2,])
     gainvals[6,] <- c(0,6,NA,NA,NA,NA) #since same as Ancova
     gainvals[7,] <- c(0,7,summary(lm(post - prior ~ treat + x3))$coef[2,])
     gainvals[8,] <- c(0,8,summary(lm(post - prior ~ treat + o1))$coef[2,])
     gainvals[9,] <- c(0,9,summary(lm(post - prior ~ treat + o2))$coef[2,])
     gainvals[10,] <- c(0,10,summary(lm(post - prior ~ treat + 
                                          o3))$coef[2,])
     #note prior below
     gainvals[11,] <- c(0,11,summary(lm(post - prior ~ treat + 
                    p1 + p2 + p3 + x1 + x3 + o1 + o2 + o3))$coef[2,])
     gainvals[12,] <- c(0,12,summary(lm(post - prior ~ treat + 
                                          know))$coef[2,])
        return(gainvals) }

anc <- function(p1,p2,p3,x1,prior,x3,o1,o2,o3,treat,post,know){
     ancvals <- matrix(ncol=6,nrow=12)
     ancvals[1,] <- c(1,1,summary(lm(post ~ treat))$coef[2,])
     ancvals[2,] <- c(1,2,summary(lm(post ~ treat + p1))$coef[2,])
     ancvals[3,] <- c(1,3,summary(lm(post ~ treat + p2))$coef[2,])
     ancvals[4,] <- c(1,4,summary(lm(post ~ treat + p3))$coef[2,])
     ancvals[5,] <- c(1,5,summary(lm(post ~ treat + x1))$coef[2,])
     ancvals[6,] <- c(1,6,summary(lm(post ~ treat + prior))$coef[2,])
     ancvals[7,] <- c(1,7,summary(lm(post ~ treat + x3))$coef[2,])
     ancvals[8,] <- c(1,8,summary(lm(post ~ treat + o1))$coef[2,])
     ancvals[9,] <- c(1,9,summary(lm(post ~ treat + o2))$coef[2,])
     ancvals[10,] <- c(1,10,summary(lm(post ~ treat + o3))$coef[2,])
     ancvals[11,] <- c(1,11,summary(lm(post ~ treat + 
          p1 + p2 + p3 + x1 + prior + x3 + o1 + o2 + o3))$coef[2,])
     ancvals[12,] <- c(1,12,summary(lm(post ~ treat + know))$coef[2,])
          return(ancvals) }


pm <- function(p1,p2,p3,x1,prior,x3,o1,o2,o3,treat,post,know){  
       pmvals <- matrix(ncol=6,nrow=12)
       pmvals[1,] <- c(2,1,NA,NA,NA,NA)
       propval <- glm(treat~ p1,family="binomial")$fitted
       mod <-   Match(post,treat,propval,estimand="ATE",
                      BiasAdjust=TRUE,ties=TRUE)
       pmvals[2,] <- c(2,2,mod$est,mod$se,
                       zval<-mod$est/mod$se,(1 - pnorm(abs(zval)))*2)

       propval <- glm(treat~ p2,family="binomial")$fitted
       mod <-   Match(post,treat,propval,estimand="ATE",
                      BiasAdjust=TRUE,ties=TRUE)
       pmvals[3,] <- c(2,3,mod$est,mod$se,
                       zval<-mod$est/mod$se,(1 - pnorm(abs(zval)))*2)

       propval <- glm(treat~ p3,family="binomial")$fitted
       mod <-   Match(post,treat,propval,estimand="ATE",
                      BiasAdjust=TRUE,ties=TRUE)
       pmvals[4,] <- c(2,4,mod$est,mod$se,
                       zval<-mod$est/mod$se,(1 - pnorm(abs(zval)))*2)

       propval <- glm(treat~ x1,family="binomial")$fitted
       mod <-   Match(post,treat,propval,estimand="ATE",
                      BiasAdjust=TRUE,ties=TRUE)
       pmvals[5,] <- c(2,5,mod$est,mod$se,
                       zval<-mod$est/mod$se,(1 - pnorm(abs(zval)))*2)

       propval <- glm(treat~ prior,family="binomial")$fitted
       mod <-   Match(post,treat,propval,estimand="ATE",
                      BiasAdjust=TRUE,ties=TRUE)
       pmvals[6,] <- c(2,6,mod$est,mod$se,
                       zval<-mod$est/mod$se,(1 - pnorm(abs(zval)))*2)

       propval <- glm(treat~ x3,family="binomial")$fitted
       mod <-   Match(post,treat,propval,estimand="ATE",
                      BiasAdjust=TRUE,ties=TRUE)
       pmvals[7,] <- c(2,7,mod$est,mod$se,
                       zval<-mod$est/mod$se,(1 - pnorm(abs(zval)))*2)

       propval <- glm(treat~ o1,family="binomial")$fitted
       mod <-   Match(post,treat,propval,estimand="ATE",
                      BiasAdjust=TRUE,ties=TRUE)
       pmvals[8,] <- c(2,8,mod$est,mod$se,
                       zval<-mod$est/mod$se,(1 - pnorm(abs(zval)))*2)

       propval <- glm(treat~ o2,family="binomial")$fitted
       mod <-   Match(post,treat,propval,estimand="ATE",
                      BiasAdjust=TRUE,ties=TRUE)
       pmvals[9,] <- c(2,9,mod$est,mod$se,
                       zval<-mod$est/mod$se,(1 - pnorm(abs(zval)))*2)
  
       propval <- glm(treat~ o3,family="binomial")$fitted
       mod <-   Match(post,treat,propval,estimand="ATE",
                      BiasAdjust=TRUE,ties=TRUE)
       pmvals[10,] <- c(2,10,mod$est,mod$se,
                       zval<-mod$est/mod$se,(1 - pnorm(abs(zval)))*2)
       
       propval <- glm(treat~ p1 + p2 + p3 + x1 + prior + 
                        x3 + o1 + o2 + o3,family="binomial")$fitted
       mod <-   Match(post,treat,propval,estimand="ATE",
                      BiasAdjust=TRUE,ties=TRUE)
       pmvals[11,] <- c(2,11,mod$est,mod$se,
                       zval<-mod$est/mod$se,(1 - pnorm(abs(zval)))*2)
    
          propval <- glm(treat~ know,family="binomial")$fitted
       mod <-   Match(post,treat,propval,estimand="ATE",
                      BiasAdjust=TRUE,ties=TRUE)
       pmvals[12,] <- c(2,12,mod$est,mod$se,
                       zval<-mod$est/mod$se,(1 - pnorm(abs(zval)))*2)
    
       return(pmvals)
       }

allmodels <- function(p1,p2,p3,x1,prior,x3,o1,o2,o3,treat,post,know){ 
  dmvals <- matrix(ncol=6,nrow=(12*3) )
  dmvals[1:12,] <- gain(p1,p2,p3,x1,prior,x3,o1,o2,o3,treat,post,know)
  dmvals[13:24,] <- anc(p1,p2,p3,x1,prior,x3,o1,o2,o3,treat,post,know)
  dmvals[25:36,] <- pm(p1,p2,p3,x1,prior,x3,o1,o2,o3,treat,post,know)
  return(dmvals)
}

qrank <- function(x) 1-(rank(x)+1)/(length(x)+2)
@

The main simulation study and the extensions had $n = 200$ and there were 10,000 replications per condition. This number of replications was used so that the standard error for the $z$ value for each statistical model was approximately 0.01. The three observed variables associated with the latent variables were constructed to have population correlations of $r=.50$ with their latent variable. The contributions from \emph{Propensity} and \emph{Other} to \emph{Knowledge} are equal, and a normally distributed random error is also added. \emph{PostTest} is based on equal contributions from \emph{Other} and \emph{Knowledge} (and random error). There is no influence from \emph{Treatment} to \emph{PostTest}. Non-zero effects could be included to evaluate the power of different methods and to explore the association between the estimated and true effects. The variables (other than \emph{Treatment}) are all scaled to have a mean of zero and a standard deviation of one. The code to create the data for this study are shown in the appendix. 

<<propancgainstudy1,size="footnotesize",cache=TRUE,echo=FALSE>>=
n <- 200
reps <- 10000 #47 
set.seed <- 49811
vals <- matrix(ncol=6,nrow=reps*(12*3)) 
for (i in 1:reps){
  pph <- rnorm(n)
  p1 <- rnorm(n)
  p3 <- scale(pph + 1.0371*rnorm(n)) #
  propen <- scale(.3*pph + .2082*p1  + .2*rnorm(n))
  p2 <- scale(sqrt(.25)*propen + sqrt(.75)*rnorm(n))
  oph <- rnorm(n)
  o1 <- rnorm(n)
  o3 <- scale(oph + 1.0371*rnorm(n)) #
  other <- scale(.3*oph + .2082*o1 + .2*rnorm(n))
  o2 <- scale(sqrt(.25)*other + sqrt(.75)*rnorm(n))
  xph <- rnorm(n)
  x1 <- rnorm(n)
  x3 <- scale(xph + .71*rnorm(n)) #
  know <- scale(.85*xph + .6935*x1 + .6*other + .6*propen)
  prior <- scale(sqrt(.25)*know + sqrt(.75)*rnorm(n))
  propenval <- qrank(propen)  
  treat <- rbinom(n,1,propenval) 
  post <- scale(other + 0*treat + know + rnorm(n)) 
  vals[((i-1)*36+1):(i*36),] <- allmodels(p1,p2,p3,x1,prior,x3,
                                    o1,o2,o3,treat,post,know)
    }

colnames(vals) <- 
    c("model","statmodel","ATE","seATE","zval","pval")
vals <- as.data.frame(vals)
vals$statmodel <- 
  factor(recoder(vals$statmodel,'1:"none";2:"p1";3:"p2";4:"p3";
                 5:"x1";6:"Prior";7:"x3";8:"o1";9:"o2";10:"o3";
                 11:"all";12:"Knowledge" '),
         levels=c("none","p1","p2","p3","x1","Prior","x3",
                  "o1","o2","o3","all","Knowledge"))
vals$model <- 
  factor(recoder(vals$model,'0:"Gain";1:"Ancova";2:"Propen" '),
         levels=c("Gain","Ancova","Propen"))
@

<<valstabs,size="small",results='asis',echo=FALSE>>=
xtab2 <- tapply(vals$zval,list(vals$statmodel,vals$model),mean)
xtab2means <- colMeans(xtab2[1:11,],na.rm=TRUE)
xtabcompare <- xtab2 <- rbind(xtab2[1:11,],xtab2means,xtab2[12,])
rownames(xtab2)[12:13] <- c("Mean","Knowledge")
print(xtable(xtab2[1:12,],caption="The mean of the test statistic ($t$ or $z$) 
    of the 10,000 replications for each statistical procedure for each covariate. 
    The standard errors of these are approximately 0.01.",
     label="tab:simul1",digits=2),hline.after=c(0,11,12))

lb <- function(x) quantile(x,.025,na.rm=TRUE)
ub <- function(x) quantile(x,.975,na.rm=TRUE)
seMean <- function(x) sd(x,na.rm=TRUE)/sqrt(sum(is.na(x)==FALSE))
xtablb <- tapply(vals$zval,list(vals$statmodel,vals$model),lb)
xtabbb <- tapply(vals$zval,list(vals$statmodel,vals$model),ub)
#xtable(xtablb,caption="z LOWER bound, study 1, effect = 0",digits=3)
#xtable(xtabub,caption="z UPPER bound, study 1, effect = 0",digits=3)
xtabse <- tapply(vals$zval,list(vals$statmodel,vals$model),seMean)

#xtable(xtabse,caption="z SE of Mean, study 1, effect = 0",digits=3)
@


\subsection{Simulation Results}
Table~\ref{tab:simul1} gives the mean of the test statistic ($t$ or $z$) for the 10,000 replications for each statistical procedure for each of the eleven sets of covariates. Values above zero correspond to mean values suggesting that the treatment has a positive effect and values below zero suggest the treatment has a negative effect. Because these are simulated data it is known that the correct answer is that the treatment had no effect.

The result that stands out is that the gain score method appears good. The mean test statistics are only slightly lower than the null. The propensity matching procedure does slightly better (i.e., means closer to zero) for all choices of covariates than the Ancova procedure, but each are biased downwards for the data model in Figure~\ref{fig:datamodel2}. These tend to estimate a negative treatment effect. For the Ancova and propensity matching procedures, conditioning on \emph{Prior} does better than the other covariates. Of the covariate sets, those related to \emph{Propensity} reduces the bias the most, and then those with \emph{Knowledge}, and finally those with \emph{Other}. 

A single simulation can show that the statistical procedures can work differently, but particularly when the model includes relationships involving latent variables---where the values used to create the data are likely speculative---it is worth checking how sensitive any conclusions are to variations in the causal model. Thus, the conclusion ``the gain score method appears good'' may be premature.

\subsection{Some Extensions}
Many assumptions are made when constructing simulated data. A valuable attribute of simulations is that if people question any of the assumptions or want to test how varying different aspects affect the results, this can be done. For illustrative purposes two extensions are shown here. 

\subsubsection{Varying the Strength of the Backdoor Paths}
The first extension varies the strength of the two backdoor paths. In the first simulation it was assumed that \emph{PostTest} was equally influenced by \emph{Knowledge}, \emph{Other}, and random error, but these contributions will vary by situation so it is important to determine if any conclusions are sensitive to the relative contributions of these. Further, the two edges, $\mathit{Knowledge} \rightarrow \mathit{PostTest}$ and $\mathit{Other} \rightarrow \mathit{PostTest}$, are part of different backdoor paths. $\mathit{Knowledge} \rightarrow \mathit{PostTest}$ is part of a path that begins unblocked, but becomes partially blocked when conditioning on covariates related to $\mathit{Knowledge}$. $\mathit{Other} \rightarrow \mathit{PostTest}$ is part of a path that begins blocked because \emph{Knowledge} is a collider in this path, but is unblocked when conditioning on $\mathit{Prior}$ because it is a descendant of $\mathit{Knowledge}$. 

For this extension the \emph{PostTest} variable will be determined by:
\begin{equation*}X \; \mathit{Knowledge} + (2-X) \; \mathit{Other} + \mathcal{N}(0,1) \quad,
\end{equation*}
\noindent where $X$ varies from 0--2 so that the mid-point of this sequence is the same as with the initial simulation. When $X=0$ this means that \emph{PostTest} is influenced by \emph{Other}, but not \emph{Knowledge}. When $X=2$ \emph{PostTest} is influenced by \emph{Knowledge}, but not \emph{Other}. For this simulation $X$ is allowed to vary in 11 steps from 0 to +2, and just the model with all covariates included is reported. 


<<propancgainstudy1_19,size="footnotesize",cache=TRUE,echo=FALSE>>=
n <- 200
reps <- 10000 #47
set.seed <- 29941
vals <- matrix(ncol=6,nrow=reps*(12*3)) 
for (i in 1:reps){
  pph <- rnorm(n)
  p1 <- rnorm(n)
  p3 <- scale(pph + 1.0371*rnorm(n)) #
  propen <- scale(.3*pph + .2082*p1  + .2*rnorm(n))
  p2 <- scale(sqrt(.25)*propen + sqrt(.75)*rnorm(n))
  oph <- rnorm(n)
  o1 <- rnorm(n)
  o3 <- scale(oph + 1.0371*rnorm(n)) #
  other <- scale(.3*oph + .2082*o1 + .2*rnorm(n))
  o2 <- scale(sqrt(.25)*other + sqrt(.75)*rnorm(n))
  xph <- rnorm(n)
  x1 <- rnorm(n)
  x3 <- scale(xph + .71*rnorm(n)) #
  know <- scale(.85*xph + .6935*x1 + .6*other + .6*propen)
  prior <- scale(sqrt(.25)*know + sqrt(.75)*rnorm(n))
  propenval <- qrank(propen)  
  treat <- rbinom(n,1,propenval) 
  post <- scale(1.9*other + 0*treat + .1*know + rnorm(n)) 
  vals[((i-1)*36+1):(i*36),] <- allmodels(p1,p2,p3,x1,prior,x3,o1,o2,o3,treat,post,know)
    }

colnames(vals) <- 
    c("model","statmodel","ATE","seATE","zval","pval")
vals <- as.data.frame(vals)
vals$statmodel <- 
  factor(recoder(vals$statmodel,'1:"none";2:"p1";3:"p2";4:"p3";5:"x1";
                 6:"Prior";7:"x3";8:"o1";9:"o2";10:"o3";11:"all";12:"Knowledge" '),
         levels=c("none","p1","p2","p3","x1","Prior","x3","o1","o2","o3","all","Knowledge"))
vals$model <- 
  factor(recoder(vals$model,'0:"Gain";1:"Ancova";2:"Propen" '),levels=c("Gain","Ancova","Propen"))
@

<<valstabs19,size="small",results='asis',echo=FALSE,eval=FALSE>>=
xtab219 <- tapply(vals$zval,list(vals$statmodel,vals$model),mean)
xtab219means <- colMeans(xtab219[1:11,],na.rm=TRUE)
xtab219 <- rbind(xtab219[1:11,],xtab219means,xtab219[12,])
rownames(xtab219)[12:13] <- c("Mean of above","Knowledge")
#print(xtable(xtab219,caption="The mean of the test statistic ($t$ or $z$) 
#    of the 10,000 replications for each statistical procedure for each covariate.
#    For this extension, the path $\\mathit{Other} \\rightarrow \\mathit{PostTest}$
#    is made much stronger than $\\mathit{Knowledge} \\rightarrow \\mathit{PostTest}$.
#    The standard errors of these are approximately 0.01.",
#     label="tab:simul219",digits=3),hline.after=c(0,10,11,12,13))

lb <- function(x) quantile(x,.025,na.rm=TRUE)
ub <- function(x) quantile(x,.975,na.rm=TRUE)
seMean <- function(x) sd(x,na.rm=TRUE)/sqrt(sum(is.na(x)==FALSE))
xtablb <- tapply(vals$zval,list(vals$statmodel,vals$model),lb)
xtabbb <- tapply(vals$zval,list(vals$statmodel,vals$model),ub)
#xtable(xtablb,caption="z LOWER bound, study 1, effect = 0",digits=3)
#xtable(xtabub,caption="z UPPER bound, study 1, effect = 0",digits=3)
xtabse <- tapply(vals$zval,list(vals$statmodel,vals$model),seMean)
#xtable(xtabse,caption="z SE of Mean, study 1, effect = 0",digits=3)
@


<<propancgainstudy1_91,size="footnotesize",cache=TRUE,echo=FALSE>>=
n <- 200
reps <- 10000 #47
set.seed <- 291881
vals <- matrix(ncol=6,nrow=reps*(12*3)) 
for (i in 1:reps){
  pph <- rnorm(n)
  p1 <- rnorm(n)
  p3 <- scale(pph + 1.0371*rnorm(n)) #
  propen <- scale(.3*pph + .2082*p1  + .2*rnorm(n))
  p2 <- scale(sqrt(.25)*propen + sqrt(.75)*rnorm(n))
  oph <- rnorm(n)
  o1 <- rnorm(n)
  o3 <- scale(oph + 1.0371*rnorm(n)) #
  other <- scale(.3*oph + .2082*o1 + .2*rnorm(n))
  o2 <- scale(sqrt(.25)*other + sqrt(.75)*rnorm(n))
  xph <- rnorm(n)
  x1 <- rnorm(n)
  x3 <- scale(xph + .71*rnorm(n)) #
  know <- scale(.85*xph + .6935*x1 + .6*other + .6*propen)
  prior <- scale(sqrt(.25)*know + sqrt(.75)*rnorm(n))
  propenval <- qrank(propen)  
  treat <- rbinom(n,1,propenval) 
  post <- scale(.1*other + 0*treat + 1.9*know + rnorm(n)) 
  vals[((i-1)*36+1):(i*36),] <- allmodels(p1,p2,p3,x1,prior,x3,o1,o2,o3,treat,post,know)
    }

colnames(vals) <- 
    c("model","statmodel","ATE","seATE","zval","pval")
vals <- as.data.frame(vals)
vals$statmodel <- 
  factor(recoder(vals$statmodel,'1:"none";2:"p1";3:"p2";4:"p3";5:"x1";
                 6:"Prior";7:"x3";8:"o1";9:"o2";10:"o3";11:"all";12:"Knowledge" '),
         levels=c("none","p1","p2","p3","x1","Prior","x3","o1","o2","o3","all","Knowledge"))
vals$model <- 
  factor(recoder(vals$model,'0:"Gain";1:"Ancova";2:"Propen" '),levels=c("Gain","Ancova","Propen"))
@

<<valstabs91,size="small",results='asis',echo=FALSE,eval=FALSE>>=
xtab291 <- tapply(vals$zval,list(vals$statmodel,vals$model),mean)
xtab291means <- colMeans(xtab291[1:11,],na.rm=TRUE)
xtab291 <- rbind(xtab291[1:11,],xtab291means,xtab291[12,])
rownames(xtab219)[12:13] <- c("Mean of above","Knowledge")
#print(xtable(xtab219,caption="The mean of the test statistic ($t$ or $z$) 
#    of the 10,000 replications for each statistical procedure for each covariate.
#    For this extension, the path $\\mathit{Knowledge} \\rightarrow \\mathit{PostTest}$
#    is made much stronger than $\\mathit{Other} \\rightarrow \\mathit{PostTest}$.
#    The standard errors of these are approximately 0.01.",
#     label="tab:simul291",digits=3),hline.after=c(0,10,11,12,13))
lb <- function(x) quantile(x,.025,na.rm=TRUE)
ub <- function(x) quantile(x,.975,na.rm=TRUE)
seMean <- function(x) sd(x,na.rm=TRUE)/sqrt(sum(is.na(x)==FALSE))
xtablb <- tapply(vals$zval,list(vals$statmodel,vals$model),lb)
xtabbb <- tapply(vals$zval,list(vals$statmodel,vals$model),ub)
#xtable(xtablb,caption="z LOWER bound, study 1, effect = 0",digits=3)
#xtable(xtabub,caption="z UPPER bound, study 1, effect = 0",digits=3)
xtabse <- tapply(vals$zval,list(vals$statmodel,vals$model),seMean)
#xtable(xtabse,caption="z SE of Mean, study 1 with KNOWLEDGE being the main path, effect = 0",digits=3)
@

<<results='asis',echo=FALSE,eval=FALSE>>=
xtabtoget <- cbind(xtab219,xtab291)
print(xtable(xtabtoget,caption="combining, see how it looks ... I'll be taking this 
             output into tabular anyway so not going to play with column names ... use names from commented out tables",
     label="tab:simul21991",digits=3),hline.after=c(0,10,11,12,13))
@



<<analysismethodsscale,size='footnotesize',echo=FALSE>>=
# 0 gain, 1 ancova, 2 pm
sgain <- function(p1,p2,p3,x1,prior,x3,o1,o2,o3,treat,post,know,rats){
     gainvals <- matrix(ncol=6,nrow=1)
     gainvals[1,] <- c(0,rats,summary(lm(post - prior ~ treat + 
                p1 + p2 + p3 + x1 + x3 + o1 + o2 + o3))$coef[2,])
        return(gainvals) }

sanc <- function(p1,p2,p3,x1,prior,x3,o1,o2,o3,treat,post,know,rats){
     ancvals <- matrix(ncol=6,nrow=1)
     ancvals[1,] <- c(1,rats,summary(lm(post ~ treat + 
                        p1 + p2 + p3 + x1 + prior + x3 + o1 + o2 + o3))$coef[2,])
          return(ancvals) }

#Tr interv must be 0/1 and 1 Treatment or ATT is ATC
#not using caliper but adjusting bias
#ties=TRUE if estimating bias ... slows it down
spm <- function(p1,p2,p3,x1,prior,x3,o1,o2,o3,treat,post,know,rats){  
       pmvals <- matrix(ncol=6,nrow=1)
  
       propval <- glm(treat~ p1 + p2 + p3 + x1 + prior + x3 + o1 + o2 + o3,family="binomial")$fitted
       mod <-   Match(post,treat,propval,estimand="ATE",BiasAdjust=TRUE,ties=TRUE)
       pmvals[1,] <- c(2,rats,mod$est,mod$se,
                       zval<-mod$est/mod$se,(1 - pnorm(abs(zval)))*2)
       return(pmvals)
       }

sallmodels <- function(p1,p2,p3,x1,prior,x3,o1,o2,o3,treat,post,know,rats){ 
  dmvals <- matrix(ncol=6,nrow=(1*3) )
  dmvals[1,] <- sgain(p1,p2,p3,x1,prior,x3,o1,o2,o3,treat,post,know,rats)
  dmvals[2,] <- sanc(p1,p2,p3,x1,prior,x3,o1,o2,o3,treat,post,know,rats)
  dmvals[3,] <- spm(p1,p2,p3,x1,prior,x3,o1,o2,o3,treat,post,know,rats)
  return(dmvals)
}

qrank <- function(x) 1-(rank(x)+1)/(length(x)+2)

@


<<propancgainstudy1_scale,size="footnotesize",cache=TRUE,echo=FALSE>>=
n <- 200
reps <- 10000 #47
seqr <- 11
set.seed <- 49123
#vals <- matrix(ncol=6,nrow=reps*seqr*3) 
vals <- {}  
KnOtratio <- seq(0,2,length.out=seqr)
counter <- 1
for (i in 1:reps){
  for (j in 1:seqr){  
    rats <- KnOtratio[j]
    pph <- rnorm(n)
    p1 <- rnorm(n)
    p3 <- scale(pph + 1.0371*rnorm(n)) #
    propen <- scale(.3*pph + .2082*p1  + .2*rnorm(n))
    p2 <- scale(sqrt(.25)*propen + sqrt(.75)*rnorm(n))
    oph <- rnorm(n)
    o1 <- rnorm(n)
    o3 <- scale(oph + 1.0371*rnorm(n)) #
    other <- scale(.3*oph + .2082*o1 + .2*rnorm(n))
    o2 <- scale(sqrt(.25)*other + sqrt(.75)*rnorm(n))
    xph <- rnorm(n)
    x1 <- rnorm(n)
    x3 <- scale(xph + .71*rnorm(n)) #
    know <- scale(.85*xph + .6935*x1 + .6*other + .6*propen)
    prior <- scale(sqrt(.25)*know + sqrt(.75)*rnorm(n))
    propenval <- qrank(propen)  
    treat <- rbinom(n,1,propenval) 
    post <- scale((2-rats)*other + 0*treat + (rats)*know + rnorm(n)) 
    #vals[((i-1)*3+1):(i*j*3),] <- sallmodels(p1,p2,p3,x1,prior,x3,o1,o2,o3,treat,post,know,rats)
    #counter <- counter + 3
    vals <- rbind(vals,sallmodels(p1,p2,p3,x1,prior,x3,o1,o2,o3,treat,post,know,rats))
    }}




colnames(vals) <- 
    c("model","Rats","ATE","seATE","zval","pval")
vals <- as.data.frame(vals)
vals$model <- 
  factor(recoder(vals$model,'0:"Gain";1:"Ancova";2:"Propen" '),levels=c("Gain","Ancova","Propen"))
@

<<byratio,out.width="5in",fig.width=5*1.2,out.height="4in",fig.height=4*1.2,fig.align="center",fig.cap="Test statistic values by statistical procedure with the the relative impact of \\emph{Other} and \\emph{Knowledge} on \\emph{PostTest}. Note: Only 10,000 of the points are plotted. A random \\emph{jitter} has been added to \\emph{x}-axis values.",echo=FALSE>>=
set.seed(30239)
par(mar=c(6.3,6,1,1.5))
set.seed(23)
xyvals <- vals[sample(1:nrow(vals),10000),c(1,2,5)]
plot(jitter(xyvals[,2],factor=1.5),xyvals[,3],col=xyvals[,1],cex=.2,las=1,
     xlab="",
     ylab="", ylim=c(-5,5.3),yaxt='n',
     cex.lab=1.1,cex.axis=1.1)
axis(2,seq(-4,4,2),seq(-4,4,2),las=1,cex=1.1)
mtext("Test",2,3.4,at=1.2,las=1,cex=1.1)
mtext("Statistic",2,2.7,at=.6,las=1,cex=1.1)
mtext("Relative amount of the influence of Knowledge versus Other",1,4.7)
mtext("Just Other",1,2.1,at=0)
mtext("No Other",1,3.1,at=2)
mtext("No Knowledge",1,3.1,at=0)
mtext("Just Knowledge",1,2.1,at=2)

meanvals <- tapply(vals$zval,list(vals$model,vals$Rats),mean)
for (i in 1:3){
  lines(KnOtratio,meanvals[i,],lwd=2.2,col=i)
}
#abline(h=0) #Think this between these looks best???
abline(h=0)
for (i in 1:3){
  points(KnOtratio,meanvals[i,],cex=.8,pch=21,col=i,bg="white")
}
for (i in 1:3){
  lines(KnOtratio,meanvals[i,],lwd=.8,col="white")
}
legend("topright",c("Gain Score","Propensity","Ancova"),lty=1,col=c(1,3,2),
       bty='n',text.col=c(1,3,2))
#meanvals
#xtabcompare
@

Figure~\ref{fig:byratio} shows that the estimated treatment effect is higher, for all values of $X$, for the gain score method, followed by the propensity score methods, and then the Ancova. All underestimate the treatment effect when the effect of \emph{Knowledge} is greater than the effect of \emph{Other} (the right side of the plot, $X > 1$). When the relative effects of these two are the same ($X = 1$), as with the above simulation, the gain score method correctly finds no treatment effects. When there is no \emph{Knowledge} influence on \emph{Propensity} ($X=0$), the propensity matching and Ancova methods estimate almost no treatment effect, and the gain score method incorrectly estimates a positive treatment effect. This shows that it is important to explore different causal models to decide which may be more appropriate. If there is uncertainty, for example, in the relative impact of these two effects, it is worth reporting this sensitivity, report any empirical evidence about which causal models appear more plausible, and in some cases report multiple analyses. Conclusions should be tentative if different procedures, chosen because of their fit for plausible causal models, lead to different conclusions.

%The following not reported
<<analysismethodsscaleknow,size='footnotesize',echo=FALSE>>=
# 0 gain, 1 ancova, 2 pm
ksgain <- function(p1,p2,p3,x1,prior,x3,o1,o2,o3,treat,post,know,rats){
     gainvals <- matrix(ncol=6,nrow=1)
     gainvals[1,] <- c(0,rats,summary(lm(post - prior ~ treat + know))$coef[2,])
        return(gainvals) }

ksanc <- function(p1,p2,p3,x1,prior,x3,o1,o2,o3,treat,post,know,rats){
     ancvals <- matrix(ncol=6,nrow=1)
     ancvals[1,] <- c(1,rats,summary(lm(post ~ treat + know))$coef[2,])
          return(ancvals) }

#Tr interv must be 0/1 and 1 Treatment or ATT is ATC
#not using caliper but adjusting bias
#ties=TRUE if estimating bias ... slows it down
kspm <- function(p1,p2,p3,x1,prior,x3,o1,o2,o3,treat,post,know,rats){  
       pmvals <- matrix(ncol=6,nrow=1)
  
       propval <- glm(treat~ know,family="binomial")$fitted
       mod <-   Match(post,treat,propval,estimand="ATE",BiasAdjust=TRUE,ties=TRUE)
       pmvals[1,] <- c(2,rats,mod$est,mod$se,
                       zval<-mod$est/mod$se,(1 - pnorm(abs(zval)))*2)
       return(pmvals)
       }

ksallmodels <- function(p1,p2,p3,x1,prior,x3,o1,o2,o3,treat,post,know,rats){ 
  kdmvals <- matrix(ncol=6,nrow=(1*3) )
  kdmvals[1,] <- ksgain(p1,p2,p3,x1,prior,x3,o1,o2,o3,treat,post,know,rats)
  kdmvals[2,] <- ksanc(p1,p2,p3,x1,prior,x3,o1,o2,o3,treat,post,know,rats)
  kdmvals[3,] <- kspm(p1,p2,p3,x1,prior,x3,o1,o2,o3,treat,post,know,rats)
  return(kdmvals)
}

qrank <- function(x) 1-(rank(x)+1)/(length(x)+2)

@

<<propancgainstudy1_scalek,size="footnotesize",cache=TRUE,echo=FALSE,eval=FALSE>>=
n <- 200
reps <- 10000 #47
seqr <- 11
set.seed <- 29123
#vals <- matrix(ncol=6,nrow=reps*seqr*3) 
kvals <- {}  
KnOtratio <- seq(0,2,length.out=seqr)
counter <- 1
for (i in 1:reps){
  for (j in 1:seqr){  
    rats <- KnOtratio[j]
    pph <- rnorm(n)
    p1 <- rnorm(n)
    p3 <- scale(pph + 1.0371*rnorm(n)) #
    propen <- scale(.3*pph + .2082*p1  + .2*rnorm(n))
    p2 <- scale(sqrt(.25)*propen + sqrt(.75)*rnorm(n))
    oph <- rnorm(n)
    o1 <- rnorm(n)
    o3 <- scale(oph + 1.0371*rnorm(n)) #
    other <- scale(.3*oph + .2082*o1 + .2*rnorm(n))
    o2 <- scale(sqrt(.25)*other + sqrt(.75)*rnorm(n))
    xph <- rnorm(n)
    x1 <- rnorm(n)
    x3 <- scale(xph + .71*rnorm(n)) #
    know <- scale(.85*xph + .6935*x1 + .6*other + .6*propen)
    prior <- scale(sqrt(.25)*know + sqrt(.75)*rnorm(n))
    propenval <- qrank(propen)  
    treat <- rbinom(n,1,propenval) 
    post <- scale(rats*other + 0*treat + (2-rats)*know + rnorm(n)) 
    #vals[((i-1)*3+1):(i*j*3),] <- sallmodels(p1,p2,p3,x1,prior,x3,o1,o2,o3,treat,post,know,rats)
    #counter <- counter + 3
    kvals <- rbind(kvals,ksallmodels(p1,p2,p3,x1,prior,x3,o1,o2,o3,treat,post,know,rats))
    }}


colnames(kvals) <- 
    c("model","Rats","ATE","seATE","zval","pval")
kvals <- as.data.frame(kvals)
kvals$model <- 
  factor(recoder(kvals$model,'0:"Gain";1:"Ancova";2:"Propen" '),levels=c("Gain","Ancova","Propen"))
@

<<comparescale1k,eval=FALSE,echo=FALSE>>=
plot(kvals$Rats,kvals$zval,col=kvals$model,cex=.2)
kmeanvals <- tapply(kvals$zval,list(kvals$model,kvals$Rats),mean)
for (i in 1:3){
  points(KnOtratio,kmeanvals[i,],cex=2,col=i)
  lines(KnOtratio,kmeanvals[i,],lwd=2,col=i)
}
abline(h=0)
kmeanvals
xtabcompare
@

\subsubsection{Having the Covariate Influence the Treatment}
The second extension focuses on whether the covariate has a causal impact on treatment condition. In some situations an assessment is used both to assign people to groups (e.g., a remedial or accelerate program in education) and as a baseline measure. There are concerns using a covariate in this way, but it occurs. Suppose that \emph{Prior} influences the propensity to be assigned to the treatment condition for the graph in Figure~\ref{fig:datamodel2}. All else being equal, people with high \emph{Prior} scores will likely regress down a bit towards the sample mean on their \emph{PostTest}, and the opposite for people with low scores \citep{Galton1886}. Suppose that high \emph{Prior} scores influences whether someone is in the treatment condition. In this circumstance you would expect many of those in the treatment to regress down towards the sample mean and many of those not in the treatment condition to regress up towards the sample mean. Thus, the gain score procedure will often show that the treatment is detrimental even when the true treatment effect is zero. Suppose \emph{Prior} scores are associated, but not directly causally related to treatment allocation. Both might be caused by a third variable, like really liking math. For explanatory purposes, suppose there are two groups: math lovers and math non-lovers. Now on the \emph{PostTest} you would expect people to regress towards their groups' mean (which will likely be higher for the math lovers group). If you compare two people with similar \emph{Prior} scores from these two groups (as done with Ancova and matching procedures), you would expect the person in the math lovers group to have the higher \emph{PostTest} score because that person is regressing towards the higher mean than the person in the math non-loving group. Since there will be more math lovers in the treatment condition, you would expect the Ancova and matching procedures to show a positive treatment effects even when the true treatment effect is zero.


<<propancgainstudy3,size="footnotesize",cache=TRUE,echo=FALSE>>=
n <- 200
reps <- 10000 #47 
set.seed <- 4211355
vals <- matrix(ncol=6,nrow=reps*(12*3)) 
for (i in 1:reps){
  pph <- rnorm(n)
  p1 <- rnorm(n)
  p3 <- scale(pph + 1.0371*rnorm(n)) #
#  propen <- scale(.3*pph + .2082*p1  + .2*rnorm(n))
  p2 <- scale(sqrt(.25)*propen + sqrt(.75)*rnorm(n))
  oph <- rnorm(n)
  o1 <- rnorm(n)
  o3 <- scale(oph + 1.0371*rnorm(n)) #
  other <- scale(.3*oph + .2082*o1 + .2*rnorm(n))
  o2 <- scale(sqrt(.25)*other + sqrt(.75)*rnorm(n))
  xph <- rnorm(n)
  x1 <- rnorm(n)
  x3 <- scale(xph + .71*rnorm(n)) #
  know <- scale(.85*xph + .6935*x1 + .6*other + .6*propen)
  prior <- scale(sqrt(.25)*know + sqrt(.75)*rnorm(n))
  pph <- rnorm(n)
  p1 <- rnorm(n)
  p3 <- scale(pph + 1.0371*rnorm(n)) #
  propen <- scale(scale(.3*pph + .2082*p1  + .2*rnorm(n))+scale(prior))
  p2 <- scale(sqrt(.25)*propen + sqrt(.75)*rnorm(n))

  propenval <- qrank(propen)  
  treat <- rbinom(n,1,propenval) 
  post <- scale(other + 0*treat + know + rnorm(n)) 
  vals[((i-1)*36+1):(i*36),] <- allmodels(p1,p2,p3,x1,prior,x3,o1,o2,o3,treat,post,know)
    }

colnames(vals) <- 
    c("model","statmodel","ATE","seATE","zval","pval")
vals <- as.data.frame(vals)
vals$statmodel <- 
  factor(recoder(vals$statmodel,'1:"none";2:"p1";3:"p2";4:"p3";5:"x1";6:"Prior";7:"x3";8:"o1";9:"o2";10:"o3";11:"all";12:"Knowledge" '),
         levels=c("none","p1","p2","p3","x1","Prior","x3","o1","o2","o3","all", "Knowledge"))
vals$model <- 
  factor(recoder(vals$model,'0:"Gain";1:"Ancova";2:"Propen" '),levels=c("Gain","Ancova","Propen"))
@

<<valstabs3,size="small",results='asis',echo=FALSE,eval=FALSE>>=
xtab3 <- tapply(vals$zval,list(vals$statmodel,vals$model),mean)
xtab3means <- colMeans(xtab3[1:11,],na.rm=TRUE)
xtab3 <- rbind(xtab3[1:11,],xtab3means,xtab3[12,])
rownames(xtab3)[12:13] <- c("Mean","Knowledge")
print(xtable(xtab3,caption="The mean of the test statistic ($t$ or $z$) 
    of the 10,000 replications for each statistical procedure for each covariate.
    For this extension, the path $\\mathit{Prior} \\rightarrow \\mathit{Propensity}$
    has been added.
    The standard errors of these are approximately 0.01.",
     label="tab:simul291",digits=3),hline.after=c(0,10,11,12,13))

lb <- function(x) quantile(x,.025,na.rm=TRUE)
ub <- function(x) quantile(x,.975,na.rm=TRUE)
seMean <- function(x) sd(x,na.rm=TRUE)/sqrt(sum(is.na(x)==FALSE))
xtablb <- tapply(vals$zval,list(vals$statmodel,vals$model),lb)
xtabbb <- tapply(vals$zval,list(vals$statmodel,vals$model),ub)
#xtable(xtablb,caption="z LOWER bound, study 1, effect = 0",digits=3)
#xtable(xtabub,caption="z UPPER bound, study 1, effect = 0",digits=3)
xtabse <- tapply(vals$zval,list(vals$statmodel,vals$model),seMean)
#xtable(xtabse,caption="z SE of Mean, prior to propen study 1, effect = 0",digits=3)
@

When building extensions to evaluate statistical procedures for causal models it can be useful to add edges to the graphs, but it is also often useful to simplify the graph to isolate why the differences occur. A series of simple models are depicted in Figure~\ref{fig:datasimple}. 
In Panel A both $\mathit{Knowledge} \rightarrow \mathit{Propensity}$ and $\mathit{Prior} \rightarrow \mathit{Propensity}$ exist.  Here, the gain score procedure estimates a positive treatment effect when the true treatment effect is zero, but the other procedures estimate a negative treatment effect (Table~\ref{tab:ext3}). They are biased in different directions, and the results for the next two panels show why. In Panel B \emph{Prior} no longer affects \emph{Treatment}, and the gain score procedure provides unbiased estimates of the treatment effect but the Ancova and matching procedures estimate a negative treatment effect. Panel C has only \emph{Prior} (and random variation) influencing \emph{Propensity}. Now the Ancova and propensity matching procedures provide unbiased estimates, and the gain score procedure incorrectly estimates a positive treatment effect. 

\begin{figure}
\centering
\begin{tikzpicture}[scale=.8]
  \node[scale=1.1] (A) at (-1.9,10) {\textbf{A}};
  \node[scale=.9,draw,ellipse] (know) at (1.8,9) {\emph{Knowledge}};
  \node[scale=.9,draw,ellipse] (propen) at (-0.2,6) {\emph{Propensity}};
  \node[scale=.9] (treat) at (-.2,4.5) {\emph{Treatment}};
  \node[scale=.9] (prior) at (1.8,7.3) {\emph{Prior}};
  \node[scale=.9] (post) at (1.8,3) {\emph{PostTest}};
  \node[scale=1.2] (spacebottom) at (1,2.5) {\phantom{Hello}};
  
  \draw[shorten >=0.0cm,shorten <=.0cm,->](propen) -- (treat);
  \draw[shorten >=0.1cm,shorten <=.0cm,->](know) -- (propen);
  \draw[shorten >=0.1cm,shorten <=.0cm,->](prior) -- (propen);
  \draw[shorten >=0.0cm,shorten <=.0cm,->](know) -- (prior);
  \draw[shorten >=0.0cm,shorten <=.0cm,->](treat) -- (post);
  \draw[shorten >=0.0cm,shorten <=.0cm,->](know) to[out=-45,in=60,] (post);

  \node[scale=1.2] (B) at (4.5,10) {\textbf{B}};
  \node[scale=.9,draw,ellipse] (knowb) at (8.2,9) {\emph{Knowledge}};
  \node[scale=.9,draw,ellipse] (propenb) at (6.2,6) {\emph{Propensity}};
  \node[scale=.9] (treatb) at (6.2,4.5) {\emph{Treatment}};
  \node[scale=.9] (priorb) at (8.2,7.3) {\emph{Prior}};
  \node[scale=.9] (postb) at (8.2,3) {\emph{PostTest}};
  
  \draw[shorten >=0.0cm,shorten <=.0cm,->](propenb) -- (treatb);
  \draw[shorten >=0.1cm,shorten <=.0cm,->](knowb) -- (propenb);
  \draw[shorten >=0.0cm,shorten <=.0cm,->](knowb) -- (priorb);
  \draw[shorten >=0.0cm,shorten <=.0cm,->](treatb) -- (postb);
  \draw[shorten >=0.0cm,shorten <=.0cm,->](knowb) to[out=-45,in=60,] (postb);

  \node[scale=1.2] (C) at (10.9,10) {\textbf{C}};
  \node[scale=.9,draw,ellipse] (knowc) at (14.6,9) {\emph{Knowledge}};
  \node[scale=.9,draw,ellipse] (propenc) at (12.6,6) {\emph{Propensity}};
  \node[scale=.9] (treatc) at (12.6,4.5) {\emph{Treatment}};
  \node[scale=.9] (priorc) at (14.6,7.3) {\emph{Prior}};
  \node[scale=.9] (postc) at (14.6,3) {\emph{PostTest}};
  
  \draw[shorten >=0.0cm,shorten <=.0cm,->](propenc) -- (treatc);
%  \draw[shorten >=0.1cm,shorten <=.0cm,->](knowc) -- (propenc);
  \draw[shorten >=0.0cm,shorten <=.0cm,->](knowc) -- (priorc);
  \draw[shorten >=0.0cm,shorten <=.0cm,->](treatc) -- (postc);
  \draw[shorten >=0.0cm,shorten <=.0cm,->](knowc) to[out=-45,in=60,] (postc);
  \draw[shorten >=0.1cm,shorten <=.0cm,->](priorc) -- (propenc);

\end{tikzpicture}
\caption{\label{fig:datasimple} Three ways in which \emph{Knowledge} and \emph{Prior} may influence \emph{Propensity}. In Panel A both influence it, in Panel B only \emph{Knowledge} influences it, and in Panel C only \emph{Prior} influences it.}
\end{figure}

<<analysismethodssimple,size='footnotesize',echo=FALSE>>=
# 0 gain, 1 ancova, 2 pm
simpallmodels <- function(prior,treata,treatb,treatc,
                          posta,postb,postc){
  modvals <- matrix(nrow=12,ncol=6)
  modvals[1,] <- c(0,1,summary(lm(posta - prior ~ treata))$coef[2,])
  modvals[2,] <- c(1,1,summary(lm(postb - prior ~ treatb))$coef[2,])
  modvals[3,] <- c(2,1,summary(lm(postc - prior ~ treatc))$coef[2,])
  modvals[10,] <- c(0,4,summary(lm(posta ~ treata))$coef[2,])
  modvals[11,] <- c(1,4,summary(lm(postb ~ treatb))$coef[2,])
  modvals[12,] <- c(2,4,summary(lm(postc ~ treatc))$coef[2,])
  modvals[4,] <- c(0,2,summary(lm(posta ~ treata + prior))$coef[2,])
  modvals[5,] <- c(1,2,summary(lm(postb ~ treatb + prior))$coef[2,])
  modvals[6,] <- c(2,2,summary(lm(postc ~ treatc + prior))$coef[2,])
    propval <- glm(treata ~ prior,family="binomial")$fitted
    mod <- Match(posta,treata,propval,estimand="ATE",BiasAdjust=TRUE,ties=TRUE)
  modvals[7,] <- c(0,3,mod$est,mod$se,
           zval<-mod$est/mod$se,(1 - pnorm(abs(zval)))*2)
    propval <- glm(treatb ~ prior,family="binomial")$fitted
    mod <- Match(postb,treatb,propval,estimand="ATE",BiasAdjust=TRUE,ties=TRUE)
  modvals[8,] <- c(1,3,mod$est,mod$se,
           zval<-mod$est/mod$se,(1 - pnorm(abs(zval)))*2)
    propval <- glm(treatc ~ prior,family="binomial")$fitted
    mod <- Match(postc,treatc,propval,estimand="ATE",BiasAdjust=TRUE,ties=TRUE)
  modvals[9,] <- c(2,3,mod$est,mod$se,
           zval<-mod$est/mod$se,(1 - pnorm(abs(zval)))*2)
    return(modvals)
       }
qrank <- function(x) 1-(rank(x)+1)/(length(x)+2)
@


<<propancgainstudy3simpler,size="footnotesize",cache=TRUE,echo=FALSE>>=
n <- 200
reps <- 10000 #47 
set.seed <- 9384
#3*4 is a/b/c by 4 procedures
simpvals <- matrix(ncol=6,nrow=reps*12) 
for (i in 1:reps){
  know <- rnorm(n)
  prior <- scale(know + rnorm(n))
  propena <- scale(know + prior + rnorm(n))  
  propenb <- scale(know + rnorm(n))  
  propenc <- scale(prior + rnorm(n))  
  treata <- rbinom(n,1,qrank(propena))
  treatb <- rbinom(n,1,qrank(propenb))
  treatc <- rbinom(n,1,qrank(propenc))
  #posts differ just by rnorm, but two kept in case
  #treatment effect added by someone
  posta <- scale(0*treata + know + rnorm(n)) 
  postb <- scale(0*treatb + know + rnorm(n)) 
  postc <- scale(0*treatc + know + rnorm(n)) 
  simpvals[((i-1)*12+1):(i*12),] <- 
    simpallmodels(prior,treata,treatb,treatc,posta,postb,postc)
    }

colnames(simpvals) <- 
    c("DAG","Procedure","ATE","seATE","zval","pval")
simpvals <- as.data.frame(simpvals)
simpvals$DAG <- 
  factor(recoder(simpvals$DAG,'0:"A";1:"B";2:"C" '),
         levels=c("A","B","C"))
simpvals$Procedure <- 
  factor(recoder(simpvals$Procedure,
          '1:"Gain";2:"Ancova";3:"Propensity Matching";4:"t-test" '),
         levels=c("Gain","Ancova","Propensity Matching", "t-test"))
@

<<tab:ext3,results='asis',echo=FALSE>>=
xtab <- with(simpvals,tapply(zval,list(DAG,Procedure),mean))
#xtable(xtab[,1:3],
#       caption="covariate influencing treatment",
#       label="tab:ext3")
@
\begin{table}
\begin{center}
\caption{The mean test statistic for the three statistical procedures for each panel in Figure~\ref{fig:datasimple}.\\} \label{tab:ext3}
\begin{tabular}{l c c c c}
&& \multicolumn{3}{c}{Statistical Procedure}\\
& & Gain & Ancova & Prop. Matching}\\
\cline{3-5} 
\multirow{3}{*}{Panel} 
& A 
& \Sexpr{sprintf("%1.2f",xtab[1,1])}
& \Sexpr{sprintf("%1.2f",xtab[1,2])}
& \Sexpr{sprintf("%1.2f",xtab[1,3])} \\
& B 
& \Sexpr{sprintf("%1.2f",xtab[2,1])}
& \Sexpr{sprintf("%1.2f",xtab[2,2])}
& \Sexpr{sprintf("%1.2f",xtab[2,3])} \\
& C 
& \Sexpr{sprintf("%1.2f",xtab[3,1])}
& \Sexpr{sprintf("%1.2f",xtab[3,2])}
& \Sexpr{sprintf("%1.2f",xtab[3,3])} \\
\hline
\end{tabular}
\end{center}
\end{table}

\section{Conclusion}
Gain score, Ancova, and propensity matching are all used with the hope of isolating the direct effect of an intervention on an outcome measure. No statistical procedure can guarantee to isolate this direct effect in all situations, but each can be useful depending on the causal model that led to the data. None of these--or any statistical procedure--will work in all situations. When more effects are added to the causal/data model it can become impossible to block all backdoor paths with any statistical procedure. There are situations where no statistical procedure can accurately estimate a treatment effect. The goal is to choose which procedure is most accurate and to warn readers about this limitation.

The results here show that there are situations where the gain score model does better than the Ancova and propensity matching procedures, and situations where it does worse. The propensity matching procedures tended to outperform Ancova methods here. The choice of covariates is important. Choosing those associated with propensity performed relatively well, as did using \emph{Prior}. The main conclusion, however, is that the choice of statistical procedure depends on assumptions about how the data arose.

Unfortunately the analyst will not know the true causal model underlying the data. Assumptions must be made in order to make causal conclusions \citep{Cartwright2014}. The analyst can try several causal models, as was done with the extensions here, to test if the statistical approach is sensitive to these changes. The extensions here were specifically chosen to show that the procedures can be sensitive to changes, but not all changes have these effects. Researchers should be prepared for reviewers and readers to propose their own causal models. Enough information should be provided to allow these peers to simulate data to show if the statistical procedure used would be appropriate for their choice of causal model. When there are differences, it is often necessary to conduct further research designed to evaluate which set of causal models is more appropriate.

This can be time-consuming. Statistical procedures for intervention studies without random allocation are not simple, hence this special issue. Plotting the data and exploratory/descriptive data analysis may be useful, though the choice of these is often influenced by the assumptions made. If you are uncertain which procedure to use, you can use multiple approaches, but must describe this in the write-up (i.e., not use each and just report the one with the lowest $p$-value). \citet{SteegenEA2016} take this to an extreme advising analysts to run many potential models and then report the distribution of results and weight them for plausibility. Here, the advice is to try to limit the choice to a small number of plausible models. 


\bibliographystyle{apacite}
\bibliography{../../AllRefs}

\clearpage
\begin{center}
{\large\bf Appendix}
\end{center}
\subsection*{\textsf{R} Code for the simulations}
The code to create the data used in the main simulation is:

\begin{singlespacing}
<<propancgainstudy1,eval=FALSE,echo=TRUE>>=
@
\end{singlespacing}

The function \texttt{allmodels} runs all the statistical models and returns a six-column matrix of: the procedure (gain score, Ancova, propensity matching), which covariates are included, the effect estimate, its standard error, the $t$ (from \texttt{lm}) or $z$ (from \texttt{glm} and \texttt{Match}), and the associated two-tailed $p$-value. It calls three functions, \texttt{gain}, \texttt{anc}, and \texttt{pm}, which run the models for the gain score, Ancova, and propensity matching procedures, respectively. The package \textbf{Matching} \citep{Matching} needs to be installed and loaded since it is called by the \texttt{pm} function. This function and the functions it calls are here:

\begin{singlespacing}
<<allmodels,eval=FALSE,echo=TRUE>>=
@
\end{singlespacing}

The code for these, the extensions, and the entire paper are also available at: \url{https://github.com/MASKED}. Readers are encouraged to adapt the code for their own needs. 
\end{document}
